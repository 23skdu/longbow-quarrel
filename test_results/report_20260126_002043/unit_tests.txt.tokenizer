=== RUN   TestTokenizerEncode
GGUF: Version=3, Tensors=0, KV=1
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 213
DEBUG: Computed Padding Offset: 224
DEBUG: Data Start Offset: 224
=== RUN   TestTokenizerEncode/simple_hello_world
=== RUN   TestTokenizerEncode/with_punctuation
=== RUN   TestTokenizerEncode/multi_word_-_single_tokens
=== RUN   TestTokenizerEncode/empty_string
--- PASS: TestTokenizerEncode (0.00s)
    --- PASS: TestTokenizerEncode/simple_hello_world (0.00s)
    --- PASS: TestTokenizerEncode/with_punctuation (0.00s)
    --- PASS: TestTokenizerEncode/multi_word_-_single_tokens (0.00s)
    --- PASS: TestTokenizerEncode/empty_string (0.00s)
=== RUN   TestTokenizerEncodeWithMerges
GGUF: Version=3, Tensors=0, KV=2
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 401
DEBUG: Computed Padding Offset: 416
DEBUG: Data Start Offset: 416
--- PASS: TestTokenizerEncodeWithMerges (0.00s)
=== RUN   TestTokenizerVocabLookup
GGUF: Version=3, Tensors=0, KV=1
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 122
DEBUG: Computed Padding Offset: 128
DEBUG: Data Start Offset: 128
=== RUN   TestTokenizerVocabLookup/<unk>
=== RUN   TestTokenizerVocabLookup/test
=== RUN   TestTokenizerVocabLookup/token
=== RUN   TestTokenizerVocabLookup/ization
=== RUN   TestTokenizerVocabLookup/unknown
--- PASS: TestTokenizerVocabLookup (0.00s)
    --- PASS: TestTokenizerVocabLookup/<unk> (0.00s)
    --- PASS: TestTokenizerVocabLookup/test (0.00s)
    --- PASS: TestTokenizerVocabLookup/token (0.00s)
    --- PASS: TestTokenizerVocabLookup/ization (0.00s)
    --- PASS: TestTokenizerVocabLookup/unknown (0.00s)
=== RUN   TestTokenizerSpaceHandling
GGUF: Version=3, Tensors=0, KV=1
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 141
DEBUG: Computed Padding Offset: 160
DEBUG: Data Start Offset: 160
--- PASS: TestTokenizerSpaceHandling (0.00s)
=== RUN   TestTokenizerEdgeCases
GGUF: Version=3, Tensors=0, KV=1
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 109
DEBUG: Computed Padding Offset: 128
DEBUG: Data Start Offset: 128
=== RUN   TestTokenizerEdgeCases/single_character
=== RUN   TestTokenizerEdgeCases/repeated_character
=== RUN   TestTokenizerEdgeCases/unknown_characters
=== RUN   TestTokenizerEdgeCases/mixed_known_and_unknown
=== RUN   TestTokenizerEdgeCases/whitespace_only
=== RUN   TestTokenizerEdgeCases/unicode
--- PASS: TestTokenizerEdgeCases (0.00s)
    --- PASS: TestTokenizerEdgeCases/single_character (0.00s)
    --- PASS: TestTokenizerEdgeCases/repeated_character (0.00s)
    --- PASS: TestTokenizerEdgeCases/unknown_characters (0.00s)
    --- PASS: TestTokenizerEdgeCases/mixed_known_and_unknown (0.00s)
    --- PASS: TestTokenizerEdgeCases/whitespace_only (0.00s)
    --- PASS: TestTokenizerEdgeCases/unicode (0.00s)
=== RUN   TestTokenizer_Mistral_RealLoad
GGUF: Version=3, Tensors=291, KV=25
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 757690
DEBUG: Computed Padding Offset: 757696
DEBUG: Data Start Offset: 757696
    tokenizer_mistral_test.go:23: Loaded 32768 tokens
--- PASS: TestTokenizer_Mistral_RealLoad (0.01s)
=== RUN   TestTokenizer_Mistral_DecodeSpecific
GGUF: Version=3, Tensors=291, KV=25
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 757690
DEBUG: Computed Padding Offset: 757696
DEBUG: Data Start Offset: 757696
    tokenizer_mistral_test.go:35: Token 31980 decodes to: "\u2063"
    tokenizer_mistral_test.go:35: Token 9445 decodes to: "ogy"
--- PASS: TestTokenizer_Mistral_DecodeSpecific (0.01s)
=== RUN   TestTokenizer_Mistral_SpecialTokens
GGUF: Version=3, Tensors=291, KV=25
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 757690
DEBUG: Computed Padding Offset: 757696
DEBUG: Data Start Offset: 757696
    tokenizer_mistral_test.go:53: BOS (1): "<s>"
    tokenizer_mistral_test.go:54: EOS (2): "</s>"
--- PASS: TestTokenizer_Mistral_SpecialTokens (0.01s)
=== RUN   TestTokenizer_Mistral_RoundTrip
GGUF: Version=3, Tensors=291, KV=25
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 757690
DEBUG: Computed Padding Offset: 757696
DEBUG: Data Start Offset: 757696
    tokenizer_mistral_test.go:70: Input: "The capital of France is Paris."
    tokenizer_mistral_test.go:71: IDs: [1782 6333 1070 5611 1117 6233 29491]
    tokenizer_mistral_test.go:72: Decoded: "The capital of France is Paris."
--- PASS: TestTokenizer_Mistral_RoundTrip (0.01s)
=== RUN   TestTokenizer_Mistral_PromptFormat
GGUF: Version=3, Tensors=291, KV=25
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 757690
DEBUG: Computed Padding Offset: 757696
DEBUG: Data Start Offset: 757696
    tokenizer_mistral_test.go:91: Prompt: "[INST] What is the capital of France? [/INST]"
    tokenizer_mistral_test.go:92: IDs: [3 2592 1117 1040 6333 1070 5611 29572 1501 29516 17057 29561]
    tokenizer_mistral_test.go:101: Decoded: "[INST] What is the capital of France? [/INST]"
--- PASS: TestTokenizer_Mistral_PromptFormat (0.01s)
=== RUN   TestTokenizerDecode
GGUF: Version=3, Tensors=0, KV=1
DEBUG: Alignment = 32
DEBUG: Offset BEFORE padding: 126
DEBUG: Computed Padding Offset: 128
DEBUG: Data Start Offset: 128
--- PASS: TestTokenizerDecode (0.00s)
PASS
ok  	github.com/23skdu/longbow-quarrel/internal/tokenizer	(cached)
